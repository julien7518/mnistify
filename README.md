# Mnistify

![App screenshot](public/og-image.png)

Live demo: https://mnistify.vercel.app

## Aper√ßu

Bienvenue sur Mnistify, un petit projet qui rend le machine learning amusant et accessible directement dans ton navigateur. Le but √©tait de cr√©er une exp√©rience interactive o√π tu peux jouer avec la reconnaissance de chiffres (dataset MNIST) en utilisant diff√©rents mod√®les de deep learning.

Tout se passe c√¥t√© client gr√¢ce √† WebGPU - donc pas de serveur co√ªteux, juste ton GPU qui travaille !

## Fonctionnalit√©s

üé® **Interface intuitive**

- Dessine tes propres chiffres
- Visualisation en temps r√©el des pr√©dictions

üìä **Visualisation des performances**

- Graphiques interactifs des pr√©dictions
- Visualisation de l'entr√©e du model
- Comparaison des temps d'inf√©rence entre mod√®les

üîÑ **Mod√®les disponibles**

- MLP (Multi-Layer Perceptron) : rapide et l√©ger
- CNN (Convolutional Neural Network) : plus pr√©cis

## üöÄ Tech Stack

### üéØ Frontend

- **[React](https://react.dev/)** ‚Äì Biblioth√®que JavaScript.
- **[Next.js](https://nextjs.org/)** ‚Äì Framework React pour applications web.
- **[shadcn/ui](https://ui.shadcn.com/)** ‚Äì Biblioth√®que de composants UI.
- **[Tailwind CSS](https://tailwindcss.com/)** ‚Äì Framework CSS.
- **[Recharts](https://recharts.org/)** ‚Äì Biblioth√®que de visualisation de donn√©es bas√©e sur React.

### üß† Machine Learning

- **[Python 3.13.4](https://docs.python.org/3.13/)**
- **[TinyGrad](https://github.com/tinygrad/tinygrad)** ‚Äì Framework de deep learning.
- **[WebGPU](https://www.w3.org/TR/webgpu/)** ‚Äì API pour inf√©rence c√¥t√© client.
- **[SafeTensors](https://huggingface.co/docs/safetensors/index)** ‚Äì Format s√©curis√© pour le partage de mod√®les.

## R√©sum√© des mod√®les

### Mod√®le utilis√© pour le MLP

| Type de couche | D√©tails                            |
| -------------- | ---------------------------------- |
| Entr√©e         | 784 neurones (image aplatie en 1D) |
| Couche dense 1 | 512 neurones avec activation SiLU  |
| Couche dense 2 | 512 neurones avec activation SiLU  |
| Sortie         | 10 neurones                        |

| Batch | LR   | Steps | LR Decay | Patience |
| ----- | ---- | ----- | -------- | -------- |
| 4096  | 0.02 | 1000  | 0.9      | 50       |

**Pr√©cision finale** : 99.14%

### Mod√®le utilis√© pour le CNN

| Type de couche | D√©tails                                   |
| -------------- | ----------------------------------------- |
| Entr√©e         | Image 1 canal (28√ó28 pixels)              |
| Convolution 1  | 32 filtres de taille 5√ó5, activation SiLU |
| Convolution 2  | 32 filtres de taille 5√ó5, activation SiLU |
| Normalisation  | Normalisation par lots (32 canaux)        |
| Pooling        | Max-pooling (r√©duction de taille)         |
| Convolution 3  | 64 filtres de taille 3√ó3, activation SiLU |
| Convolution 4  | 64 filtres de taille 3√ó3, activation SiLU |
| Normalisation  | Normalisation par lots (64 canaux)        |
| Pooling        | Max-pooling (r√©duction de taille)         |
| Aplatissement  | Conversion en vecteur 1D                  |
| Couche dense   | 576 neurones vers 10 neurones             |

| Batch | LR    | Steps | LR Decay | Patience |
| ----- | ----- | ----- | -------- | -------- |
| 256   | 0.005 | 500   | 0.9      | 50       |

**Pr√©cision finale** : 99.25%

### R√©flexions sur les tests et recherches

De nombreux tests et recherches ont √©t√© r√©alis√©s pour le CNN. Au final, sur un dataset aussi petit, le MLP avec de bons param√®tres (facilement trouvables) s'av√®re tout aussi performant tout en √©tant moins gourmand en ressources √† l'entra√Ænement. Les architectures de CNN test√©es proviennent de propositions d'architectures g√©n√©r√©es par des LLM. Nous avons commenc√© par une recherche exhaustive (grid search), mais celle-ci s'est r√©v√©l√©e trop co√ªteuse en temps et en ressources. Nous sommes donc pass√©s √† une approche _forced+random search_, bien qu'il existe d'autres m√©thodes qui auraient pu √™tre plus adapt√©es. Dans une d√©marche scientifique, il aurait √©t√© plus judicieux d'√©valuer l'impact de chaque param√®tre individuellement au lieu d'utiliser le random search, afin de comprendre pleinement l'influence de chaque hyperparam√®tre et tendre vers un optimum.

### Journal d'hyperparam√®tres

Voir [`HYPERPARAMETERS-CNN.md`](/HYPERPARAMETERS-CNN.md) pour les d√©tails de l'entra√Ænement du CNN.
Voir [`HYPERPARAMETERS-MLP.md`](/HYPERPARAMETERS-MLP.md) pour les d√©tails de l'entra√Ænement du MLP.

## Installation & ex√©cution locale

Pr√©-requis

- Node.js (recommand√© >= 18)
- Python 3.9+ (si vous voulez r√©-entra√Æner les mod√®les)
- Un environnement WebGPU compatible (navigateur r√©cent Chrome/Edge/Firefox Nightly avec drapeau WebGPU si n√©cessaire)

Frontend

```bash
# √† la racine du projet
npm install
npm run dev
```

Ouvrez http://localhost:3000 pour voir l'application.

Exemple pour un entra√Ænement du CNN :

```bash
cd python
python -m pip install -r requirements.txt
python train_model.py --model cnn
```

```bash
python train_model.py -h
```

Pour avoir le d√©tails des options possibles.
